{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOYicEfp/9HKdt/KJcE7qG8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"tsZHHossGmHQ","executionInfo":{"status":"ok","timestamp":1763791658012,"user_tz":-180,"elapsed":12,"user":{"displayName":"Ali Qafarov","userId":"14354949417546284700"}}},"outputs":[],"source":["import torch\n","\n","effective_batch_size = 64\n","total_batch_size = 16\n","grad_accum_steps = effective_batch_size // total_batch_size\n","\n","# Create training loop function\n","def train_step(model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               optimizer: torch.optim.Optimizer,\n","               device):\n","\n","    # Put the model in train mode\n","    model.train()\n","\n","    # Setup train loss, and train accuracy\n","    train_loss, train_acc = 0.0, 0.0\n","\n","    # Optimizer zero_grad\n","    optimizer.zero_grad()\n","\n","    # Loop through data loader\n","    for step, batch in enumerate(dataloader):\n","\n","        # support both style -> (input_ids, attention_mask, labels) or dicts\n","        if isinstance(batch, (list, tuple)) and len(batch) == 3:\n","            input_ids, attention_mask, labels = batch\n","        else:\n","            # if batch is a dict-like (from tokenizer), unpack\n","            input_ids = batch[\"input_ids\"]\n","            attention_mask = batch[\"attention_mask\"]\n","            labels = batch[\"labels\"]\n","\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","\n","        # 1. Forward pass and add mixed_precision\n","        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n","            y_pred = model(input_ids, attention_mask)\n","            loss = loss_fn(y_pred, labels)\n","            loss = loss / grad_accum_steps # Normalize loss\n","\n","        # accumulate gradients\n","        loss.backward()\n","        train_loss += loss.detach()\n","\n","        if (step + 1) % grad_accum_steps == 0:\n","            norm = torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            # Calculate and accumulate accuracy\n","            with torch.no_grad():\n","                y_pred_class = torch.argmax(y_pred, dim=1)\n","                acc = (y_pred_class == labels).float().mean().item()\n","                train_acc += acc\n","\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    train_loss = train_loss / len(dataloader)\n","    train_acc = train_acc / (len(dataloader) / grad_accum_steps)\n","\n","    return train_loss, train_acc"]}]}