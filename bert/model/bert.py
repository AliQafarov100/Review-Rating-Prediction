# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pLlj_ZC0sdThGR67lZ0slt4q3o8HlJKa
"""

import torch
import torch.nn.functional as F
import math
import inspect
device = "cuda" if torch.cuda.is_available() else "cpu"

class Head(torch.nn.Module):
    """ one head of self-attention """
    def __init__(self, d_model, head_size, dropout=0.1):
        super().__init__()
        self.head_size = head_size
        self.key = torch.nn.Linear(d_model, head_size, bias=False)
        self.query = torch.nn.Linear(d_model, head_size, bias=False)
        self.value = torch.nn.Linear(d_model, head_size, bias=False)
        self.dropout =  torch.nn.Dropout(dropout)

        self.dropout = torch.nn.Dropout(dropout)

    def forward(self, x, mask=None):
        B,T,C = x.shape
        k = self.key(x)
        q = self.query(x)
        v = self.value(x)

        # compute attention score
        attention_score = q @ k.transpose(-2, -1) / math.sqrt(self.head_size) # (B,T,C) @ (B,C,T) -> (B,T,T)
        # fill 0 mask with super small number
        if mask is not None:
            # mask expected (B,1,1,T)
            if mask.dim() == 2:
                mask = mask.unsqueeze(1).unsqueeze(1)
            attention_score = attention_score.masked_fill(mask == 0, float("-1e9"))

        # softmax to put attention weights
        attention_score = F.softmax(attention_score, dim=-1)
        attention_score = self.dropout(attention_score)

        # output
        out = attention_score @ v # (B,T,T) @ (B,T,C) -> (B,T,C)
        return out

class MultiHeadAttention(torch.nn.Module):
    """ multiple heads of self-attention in parallel """
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0
        head_size = d_model // num_heads

        self.heads = torch.nn.ModuleList([Head(d_model, head_size, dropout) for _ in range(num_heads)])
        self.proj = torch.nn.Linear(d_model, d_model)
        self.dropout = torch.nn.Dropout(p=dropout)

    def forward(self, x, mask=None):
        out = torch.cat([h(x, mask) for h in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out

class FeedForward(torch.nn.Module):
    """ multi layer perceptron """
    def __init__(self, d_model, dropout=0.1):
        super().__init__()

        # Position-wise Feed-Forward Networks
        self.ffn = torch.nn.Sequential(
            torch.nn.Linear(d_model, d_model*4),
            torch.nn.ReLU(),
            torch.nn.Linear(d_model*4, d_model),
            torch.nn.Dropout(p=dropout)
        )

    def forward(self, x):
        return self.ffn(x)

class PositionalEncoding(torch.nn.Module):
    def __init__(self, d_model, max_len=256):
        super().__init__()

        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer("pe", pe)

    def forward(self, x):
        # x: (B, T, C)
        return self.pe[:, :x.size(1), :]

class BERTEmbedding(torch.nn.Module):
    def __init__(self, vocab_size, d_model, dropout=0.1, seq_len=256, type_vocab=2):
        super().__init__()
        self.token = torch.nn.Embedding(vocab_size, d_model)
        # self.segment = torch.nn.Embedding(type_vocab, d_model)
        self.positional = torch.nn.Embedding(seq_len, d_model)
        self.dropout = torch.nn.Dropout(p=dropout)

    def forward(self, x):
        B, T = x.shape
        tok = self.token(x)

        # create position ids [0..T-1] and expand to batch
        positions = torch.arange(T, device=device).unsqueeze(0)  # (1, T)
        pos = self.positional(positions)                           # (1, T, d_model)
        pos = pos.expand(B, -1, -1)
        out = tok + pos
        return self.dropout(out)

class EncoderBlock(torch.nn.Module):
    """implement encoder block of transformer"""
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        # multi-head attention
        self.attn = MultiHeadAttention(d_model, num_heads, dropout)

        # feef-forward network
        self.ffn = FeedForward(d_model, dropout)

        # layer normalization for attention block
        self.layer_norm_1 = torch.nn.LayerNorm(d_model)

        # layer normalization for feed-forward block
        self.layer_norm_2 = torch.nn.LayerNorm(d_model)

    def forward(self, x, mask=None):
        h = x + self.attn(self.layer_norm_1(x), mask)
        out = h + self.ffn(self.layer_norm_2(h))
        return out

class BERT(torch.nn.Module):
    """Bidirectional Encoder Representations from Transformers."""
    def __init__(self, vocab_size, d_model=768, n_layers=12, num_heads=12, dropout=0.1, seq_len=256):
        super().__init__()
        # Embeddings layer
        self.token_embd = torch.nn.Embedding(vocab_size, d_model)
        self.pos_embd = torch.nn.Embedding(seq_len, d_model)
        self.seg_embd = torch.nn.Embedding(2, d_model)

        # multi layer transformer block
        self.encoder_blocks = torch.nn.ModuleList(
            [EncoderBlock(d_model, num_heads, dropout) for _ in range(n_layers)]
        )
        self.layer_norm = torch.nn.LayerNorm(d_model)

        # Init params
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, torch.nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
        elif isinstance(module, torch.nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            

    def forward(self, token_ids, mask, segment_ids):
        B, T = token_ids.shape
        device = token_ids.device
        # put embedding
        tok = self.token_embd(token_ids)
        pos_emb = self.pos_embd(torch.arange(T, device=device).unsqueeze(0))
        seg_emb = self.seg_embd(segment_ids)
        x = tok + pos_emb + seg_emb


        # running multi layer transformer
        for encoder in self.encoder_blocks:
            x = encoder(x, mask)

        return self.layer_norm(x)
    

class BERTClassfier(torch.nn.Module):
    def __init__(self, vocab_size, num_classes, d_model=768, n_layers=12, num_heads=12, seq_len=256):
        super().__init__()
        self.bert = BERT(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, num_heads=num_heads, dropout=0.1, seq_len=seq_len)
        self.classifier = torch.nn.Sequential(
            torch.nn.Linear(d_model, d_model),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.1),
            torch.nn.Linear(d_model, num_classes)
        )
    def forward(self, token_ids, segment_ids, mask=None):
        x = self.bert(token_ids, mask, segment_ids)
        cls = x[:, 0, :]
        return self.classifier(cls)
    
    def configure_optimizers(self, weight_decay, learning_rate, device=device):
        # start with all of the candidate parameters (that require grad)
        param_dict = {pn: p for pn, p in self.named_parameters()}
        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}
        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.
        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.
        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]
        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]
        optim_groups = [
            {'params': decay_params, 'weight_decay': weight_decay},
            {'params': nodecay_params, 'weight_decay': 0.0}
        ]
        num_decay_params = sum(p.numel() for p in decay_params)
        num_nodecay_params = sum(p.numel() for p in nodecay_params)
        print(f"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters")
        print(f"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters")
        # Create AdamW optimizer and use the fused version if it is available
        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters
        use_fused = fused_available and 'cuda' is device
        print(f"using fused AdamW: {use_fused}")
        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.999), eps=1e-8, fused=use_fused)
        return optimizer